<!DOCTYPE html><html lang="en-GB"><head><meta charset="utf-8"><meta http-equiv="Content-Security-Policy" content="default-src 'self' https://gu.illau.me; script-src 'self' 'unsafe-inline' 'unsafe-eval' https://gu.illau.me http://www.google-analytics.com http://hnbutton.appspot.com https://platform.twitter.com https://apis.google.com http://gmarty.disqus.com; style-src 'self' 'unsafe-inline' https://gu.illau.me http://fonts.googleapis.com http://a.disquscdn.com; img-src 'self' https://gu.illau.me https://*.googleusercontent.com http://www.google-analytics.com https://raw.githubusercontent.com https://syndication.twitter.com https://*.disqus.com https://*.disquscdn.com; font-src 'self' https://gu.illau.me http://fonts.gstatic.com https://fonts.gstatic.com; object-src 'none'; frame-src https://www.youtube-nocookie.com http://hnbutton.appspot.com http://disqus.com https://apis.google.com https://accounts.google.com https://plusone.google.com http://platform.twitter.com;"><meta name="viewport" content="width=device-width,initial-scale=1"><meta name="theme-color" content="#ff8c00"><title>Augmented reality &amp; computer vision</title><meta name="description" content="I prototyped a simple augmented reality experiment using computer vision in JavaScript."><base href="https://gu.illau.me/posts/augmented-reality-computer-vision/" target="_top"><link href="/css/main.css" rel="stylesheet"><script src="/js/analytics.js" defer async></script><script src="/js/ui.js" defer async></script><link href="/manifest.webmanifest" rel="manifest"><link href="http://feeds.feedburner.com/just-a-blog" rel="alternate" type="application/rss+xml" title="A blog by Guillaume C. Marty"><link href="/favicon.ico" rel="shortcut icon" type="image/x-icon"></head><body><div class="content"><div class="inner-content"><div class="scroller"><div class="post-head group"><a href="/posts/augmented-reality-computer-vision/"><h1 class="post-title">Augmented reality &amp; computer vision</h1></a><span class="post-date">2017/02/15</span><span class="post-reading-time"></span></div><div class="post-body markdown"><blockquote>
<p><strong>tl;dr:</strong> Media capture, object detection and WebGL can be combined to hack a simple augmented reality experience. See my demo called <a href="https://gmarty.github.io/all-saints-ar/">we are all Saints</a>.</p>
</blockquote>
<p>To simplify things, augmented reality is based mainly on 2 technologies:</p>
<ul>
<li>Sensors to track the position and orientation of the user field of view (FOV)</li>
<li>Computer vision to detect objects in the user FOV</li>
</ul>
<p>Of course modern AR headsets like Hololens combine both to create the best experience possible. I explored using common devices with web technologies to create a simple AR experience. I decided to go for computer vision applied to media capture.</p>
<p><img src="https://raw.githubusercontent.com/gmarty/all-saints-ar/gh-pages/demo.jpg" alt="Screenshot of the demo"></p>
<h2><a name="object-detection" class="anchor" href="#object-detection"><span class="header-link"></span></a>Object detection</h2>
<p>There are few different libraries to detect objects in an image. Most of them seem to be based on <a href="https://en.wikipedia.org/wiki/OpenCV">OpenCV</a>, ported to JavaScript via emscripten.
I didn&#39;t spend too much time looking for a library and quickly settled for <a href="https://github.com/mtschirs/js-objectdetect">js-objectdetect</a>. It&#39;s hand-written (as opposed to converted via emscripten) so it makes easy to read, understand and debug if needed. It can detect different types of objects but I used human faces here.</p>
<p>Once set up properly, <code>js-objectdetect</code> accepts a video element as an input, so I just pass to it the one that displays the camera feed I got from <code>getUserMedia</code>.</p>
<p>It return the coordinates in pixel of the faces detected (left, top, width and height).</p>
<h2><a name="recreating-a-virtual-3d-space" class="anchor" href="#recreating-a-virtual-3d-space"><span class="header-link"></span></a>Recreating a virtual 3D space</h2>
<p>Next step is to place the faces detected in the image in a virtual 3D space by estimating their respective positions. I used A-Frame to create the 3D world because this framework is easy to use.</p>
<p>Positioning an element on the <code>x</code> and <code>y</code> axes is really easy. We need to convert the position in the (-1, 1) range, more about that later. For example a point centred in the image will have both its <code>x</code> and <code>y</code> values set to <code>0</code>. Knowing the position in pixels and the size of the video the values are easy to get (Also the <code>y</code> axis direction is reverse in the web and WebGL). For the <code>x</code> axis, half the width needs to be added so that the element is horizontally centred on the face.</p>
<p>The <code>z</code> axis is a bit trickier. It needs to get estimated and calibrated. I used the height value. I noticed that once detected my face takes at most 80% of the image height when I stand at about 50cm from the camera. The further I step back, say about 2m, the smaller my face gets, to take about 30% of the screen height.</p>
<p>I used these distances as the values for the camera frustum near and far clipping planes (<code>near</code> and <code>far</code> attributes on the <code>&lt;a-camera&gt;</code> element).</p>
<p>Then I only need to convert and clamp the height of the face detected between the % values of image height that I determined above. Once converted to the (-1, 1) range, they&#39;ll vary proportionally between 50cm and 2m. That gives me a good enough approximation of where my face is located in respect to the camera.</p>
<p>I also used the detected height to position the virtual element a few apparent centimetres on top the faces.</p>
<h2><a name="overlaying" class="anchor" href="#overlaying"><span class="header-link"></span></a>Overlaying</h2>
<p> A-Frame uses Three.js under the hood. That&#39;s what I&#39;m using to perform computations.</p>
<p>Now that I&#39;ve got the x, y and z position in the (-1, 1) range I need to unproject this vector along the active camera. That is not as complicated as it sounds:</p>
<pre><code class="lang-javascript"><div class="highlight"><pre><span class="kr">const</span> <span class="nx">pos</span> <span class="o">=</span> <span class="k">new</span> <span class="nx">THREE</span><span class="p">.</span><span class="nx">Vector3</span><span class="p">(</span><span class="nx">x</span><span class="p">,</span> <span class="nx">y</span><span class="p">,</span> <span class="nx">z</span><span class="p">).</span><span class="nx">unproject</span><span class="p">(</span><span class="nx">camera</span><span class="p">);</span>
</pre></div>
</code></pre>
<p>This returns a vector corresponding to the position in space of the object given the current FOV (i.e. camera).</p>
<p>Finally I can set the position of the A-Frame element in the virtual space that I want to super impose over faces:</p>
<pre><code class="lang-javascript"><div class="highlight"><pre><span class="nx">element</span><span class="p">.</span><span class="nx">setAttribute</span><span class="p">(</span><span class="s1">&#39;position&#39;</span><span class="p">,</span> <span class="nx">pos</span><span class="p">);</span>
</pre></div>
</code></pre>
<h2><a name="conclusion" class="anchor" href="#conclusion"><span class="header-link"></span></a>Conclusion</h2>
<p>This experiment wasn&#39;t as hard as it sounded at first and the result is quite convincing. I learnt a lot about projection and unprojection in the process. As usual the code is on GitHub as <a href="https://github.com/gmarty/all-saints-ar">all-saints-ar</a> because we are all saints!</p>
</div><div class="social"><a href="https://twitter.com/share" class="twitter-share-button">Tweet</a><div data-size="medium" data-annotation="bubble" data-width="300" class="g-plusone"></div><a href="http://news.ycombinator.com/submit" class="hn-share-button">Vote on HN</a></div><div id="comments"><div id="disqus_thread"></div></div><script>// Hacker News
(function(d, t) {
  var g = d.createElement(t),
      s = d.getElementsByTagName(t)[0];
  g.src = '//hnbutton.appspot.com/static/hn.min.js';
  s.parentNode.insertBefore(g, s);
}(document, 'script'));

// Twitter
!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0];if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src="https://platform.twitter.com/widgets.js";fjs.parentNode.insertBefore(js,fjs);}}(document,"script","twitter-wjs");

// Google Plus
(function() {
  var po = document.createElement('script'); po.async = true;
  po.src = 'https://apis.google.com/js/plusone.js';
  var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(po, s);
})();

// Disqus
var disqus_shortname = 'gmarty';
(function() {
    var dsq = document.createElement('script'); dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
})();</script></div></div></div><nav><h1 class="name"><a href="/">A blog<small>by Guillaume C. Marty</small></a></h1><div class="menu icon-menu"></div><ul class="nav-links"><li class="text-link"><a href="/about.html">about</a></li><li class="text-link"><a href="/apps.html">apps</a></li><li class="text-link"><a href="/talks.html">talks</a></li><li class="text-link"><a href="/posts.html">posts</a></li></ul><ul class="social-media"><li><a href="https://twitter.com/g_marty"><img src="/img/twitter.svg"></a></li><li><a href="https://github.com/gmarty"><img src="/img/github.svg"></a></li><li><a href="https://www.linkedin.com/in/gmarty/en"><img src="/img/linkedin.svg"></a></li><li><a href="https://www.instagram.com/gcmarty/"><img src="/img/instagram.svg"></a></li></ul></nav></body></html>